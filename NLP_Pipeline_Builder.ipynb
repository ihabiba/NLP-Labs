{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXktX5ZoO1XjxlPDeX6vJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihabiba/NLP-Labs/blob/main/NLP_Pipeline_Builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOKENIZATION"
      ],
      "metadata": {
        "id": "XeSAXSXpbJhS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqSfHeGVa0fH",
        "outputId": "c30d3770-216b-48b0-c2a6-02b9c1ebc332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tokenization ---\n",
            "Sentence Tokenization:\n",
            "['Artificial Intelligence (AI) is transforming industries worldwide.', 'With its applications in healthcare, finance, and education, AI provides innovative solutions.']\n",
            "\n",
            "Word Tokenization:\n",
            "['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'transforming', 'industries', 'worldwide', '.', 'With', 'its', 'applications', 'in', 'healthcare', ',', 'finance', ',', 'and', 'education', ',', 'AI', 'provides', 'innovative', 'solutions', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# download the sentence & word tokenizer models\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"Artificial Intelligence (AI) is transforming industries worldwide.\n",
        "With its applications in healthcare, finance, and education, AI provides innovative solutions.\"\"\"\n",
        "\n",
        "# Sentence and Word Tokenization\n",
        "def tokenize_text(text):\n",
        "    print(\"--- Tokenization ---\")\n",
        "    print(\"Sentence Tokenization:\")\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(sentences)\n",
        "\n",
        "    print(\"\\nWord Tokenization:\")\n",
        "    words = word_tokenize(text)\n",
        "    print(words)\n",
        "\n",
        "tokenize_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEMMING"
      ],
      "metadata": {
        "id": "dLTdVZDkcihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "tokenlist = word_tokenize(text)\n",
        "\n",
        "def stem_text(tokenlist):\n",
        "    print(\"--- Stemming ---\")\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokenlist]\n",
        "    print(\"Stemmed Words:\")\n",
        "    print(stemmed_words)\n",
        "\n",
        "stem_text(tokenlist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd7kJmBIbQsX",
        "outputId": "a28077ed-eec9-4f74-fbf0-6cb65813ad79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stemming ---\n",
            "Stemmed Words:\n",
            "['artifici', 'intellig', 'is', 'transform', 'industri', 'worldwid', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LEMMATIZATION"
      ],
      "metadata": {
        "id": "L8WHZl04cw89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "def lemmatize_text(words):\n",
        "    print(\"--- Lemmatization ---\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    print(\"Lemmatized Words:\")\n",
        "    print(lemmatized_words)\n",
        "\n",
        "lemmatize_text(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kgYWoFFcoXk",
        "outputId": "a19afa5a-bb67-43f9-c802-c762c72fc8cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Lemmatization ---\n",
            "Lemmatized Words:\n",
            "['Artificial', 'Intelligence', 'is', 'transforming', 'industry', 'worldwide', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STOP WORD REMOVAL"
      ],
      "metadata": {
        "id": "eJyF3mIjdNaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "def identify_stop_words(words):\n",
        "    print(\"--- Stop Words ---\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    print(\"Filtered Words (without stop words):\")\n",
        "    print(filtered_words)\n",
        "\n",
        "identify_stop_words(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qy_Z4_ddGoC",
        "outputId": "1a4ff773-48b8-4d23-a962-d62bbc5df4d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stop Words ---\n",
            "Filtered Words (without stop words):\n",
            "['Artificial', 'Intelligence', 'transforming', 'industries', 'worldwide', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS (Part-of-Speech) TAGGING"
      ],
      "metadata": {
        "id": "kX73-QLfdezV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "def pos_tagging(words):\n",
        "    print(\"--- POS Tagging ---\")\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    print(\"POS Tags:\")\n",
        "    print(pos_tags)\n",
        "\n",
        "pos_tagging(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muwvAhmudWTU",
        "outputId": "a9bc3ce6-33e7-4b1a-9f78-4a8dd18c8372"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- POS Tagging ---\n",
            "POS Tags:\n",
            "[('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('transforming', 'VBG'), ('industries', 'NNS'), ('worldwide', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEPENDENCY PARSING (with spaCy)"
      ],
      "metadata": {
        "id": "m4YZfW40dxFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    print(\"--- Dependency Parsing ---\")\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        print(f\"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}, POS: {token.pos_}\")\n",
        "\n",
        "dependency_parsing(text)\n",
        "\n",
        "# Visualize the dependency tree (works in Jupyter)\n",
        "from spacy import displacy\n",
        "displacy.render(nlp(text), style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "sQnzZ2ovdsh4",
        "outputId": "20f46aae-7422-4d5f-9607-e49345d36d4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dependency Parsing ---\n",
            "Word: Artificial, Dependency: compound, Head: Intelligence, POS: PROPN\n",
            "Word: Intelligence, Dependency: nsubj, Head: transforming, POS: PROPN\n",
            "Word: is, Dependency: aux, Head: transforming, POS: AUX\n",
            "Word: transforming, Dependency: ROOT, Head: transforming, POS: VERB\n",
            "Word: industries, Dependency: dobj, Head: transforming, POS: NOUN\n",
            "Word: worldwide, Dependency: advmod, Head: transforming, POS: ADV\n",
            "Word: ., Dependency: punct, Head: transforming, POS: PUNCT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4acd9e13d78f4cf0aa0b63cdb7c6200e-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Artificial</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Intelligence</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">transforming</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">industries</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">worldwide.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4acd9e13d78f4cf0aa0b63cdb7c6200e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAMED ENTITY RECOGNITION (NER)"
      ],
      "metadata": {
        "id": "Wlap5BWlel-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    print(\"--- Named Entity Recognition ---\")\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "named_entity_recognition(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q32gmQVeWEj",
        "outputId": "718fa1fa-986b-49a9-980a-ba67b6479a3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Named Entity Recognition ---\n",
            "Entity: Artificial Intelligence, Label: PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 — Why is Text Preprocessing Essential in Building an NLP Pipeline?\n",
        "\n",
        "#### Text preprocessing is important because it cleans and standardizes raw text before applying NLP or machine learning models. It removes noise like punctuation and stop words, and uses steps such as tokenization, stemming, and lemmatization to make text simpler and more consistent. This helps models focus on meaningful patterns and improves overall accuracy."
      ],
      "metadata": {
        "id": "H_7DPkYriiCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3.2 — GUTENBERG CORPUS ANALYSIS"
      ],
      "metadata": {
        "id": "cXall5_0hNN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download required datasets\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Step 1: Load and Explore the Dataset\n",
        "print(\"--- Available Texts in Gutenberg Corpus ---\")\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "# Select a sample text\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(\"\\n--- First 500 Characters of the Text ---\")\n",
        "print(text[:500])\n",
        "\n",
        "# Step 2: Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"\\n--- Number of Sentences ---\")\n",
        "print(len(sentences))\n",
        "print(\"--- Number of Words ---\")\n",
        "print(len(words))\n",
        "\n",
        "# Step 3: Remove Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "print(\"\\n--- First 20 Non-Stop Words ---\")\n",
        "print(filtered_words[:20])\n",
        "\n",
        "# Step 4: Word Frequency Analysis\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "print(\"\\n--- 10 Most Common Words ---\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGQ5BQIUevMl",
        "outputId": "0eafa778-3356-4a58-ec4e-56117cb0cfb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Available Texts in Gutenberg Corpus ---\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "\n",
            "--- First 500 Characters of the Text ---\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n",
            "\n",
            "--- Number of Sentences ---\n",
            "7493\n",
            "--- Number of Words ---\n",
            "191855\n",
            "\n",
            "--- First 20 Non-Stop Words ---\n",
            "['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived']\n",
            "\n",
            "--- 10 Most Common Words ---\n",
            "[('emma', 860), ('could', 836), ('would', 818), ('miss', 599), ('must', 566), ('harriet', 500), ('much', 484), ('said', 483), ('one', 447), ('weston', 437)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3.3 — EFFECT OF STOP WORD REMOVAL"
      ],
      "metadata": {
        "id": "CQLjT7xHhtXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== TASK 3.3 — EFFECT OF STOP WORD REMOVAL ========\n",
        "\n",
        "all_words = [word.lower() for word in words if word.isalpha()]\n",
        "freq_all = FreqDist(all_words)\n",
        "\n",
        "print(\"\\n--- 10 Most Common Words (WITHOUT Removing Stop Words) ---\")\n",
        "print(freq_all.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suDXZ90LhXNm",
        "outputId": "579e34ee-de63-420f-8f33-a64a536e1430"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 10 Most Common Words (WITHOUT Removing Stop Words) ---\n",
            "[('the', 5201), ('to', 5181), ('and', 4877), ('of', 4284), ('i', 3177), ('a', 3124), ('it', 2503), ('her', 2448), ('was', 2396), ('she', 2336)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 3.4 — APPLY FULL NLP PIPELINE ON A BIGGER DATASET\n"
      ],
      "metadata": {
        "id": "Nvd1AeKdjnwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Load a large text (Jane Austen’s \"Emma\")\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Load spaCy model for parsing and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# First 5000 Characters of the Text\n",
        "sample_text = text[:5000]\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(\"--- Number of Tokens ---\", len(tokens))\n",
        "print(\"Sample Tokens:\", tokens[:20])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word.lower()) for word in tokens if word.isalpha()]\n",
        "print(\"\\nSample Stems:\", stems[:20])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha()]\n",
        "print(\"\\nSample Lemmas:\", lemmas[:20])\n",
        "\n",
        "# Stop word removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered = [w for w in lemmas if w not in stop_words]\n",
        "print(\"\\nFiltered Words (No Stop Words):\", filtered[:20])\n",
        "print(\"Remaining Word Count:\", len(filtered))\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(filtered)\n",
        "print(\"\\nPOS Tags (first 15):\", pos_tags[:15])\n",
        "\n",
        "# Dependency parsing\n",
        "doc = nlp(\" \".join(filtered[:30]))\n",
        "print(\"\\nDependency Relations:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} → {token.dep_} → {token.head.text}\")\n",
        "\n",
        "# Named entity recognition\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jty081rXiLGp",
        "outputId": "f01821be-3b1b-46c3-90f6-1f576c235e2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Number of Tokens --- 992\n",
            "Sample Tokens: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
            "\n",
            "Sample Stems: ['emma', 'by', 'jane', 'austen', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', 'handsom', 'clever', 'and', 'rich', 'with', 'a', 'comfort', 'home', 'and', 'happi']\n",
            "\n",
            "Sample Lemmas: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n",
            "\n",
            "Filtered Words (No Stop Words): ['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessing', 'existence', 'lived']\n",
            "Remaining Word Count: 417\n",
            "\n",
            "POS Tags (first 15): [('emma', 'NN'), ('jane', 'NN'), ('austen', 'JJ'), ('volume', 'NN'), ('chapter', 'NN'), ('emma', 'NN'), ('woodhouse', 'IN'), ('handsome', 'JJ'), ('clever', 'NN'), ('rich', 'JJ'), ('comfortable', 'JJ'), ('home', 'NN'), ('happy', 'JJ'), ('disposition', 'NN'), ('seemed', 'VBD')]\n",
            "\n",
            "Dependency Relations:\n",
            "emma → compound → jane\n",
            "jane → nsubj → austen\n",
            "austen → nsubj → seemed\n",
            "volume → compound → emma\n",
            "chapter → compound → emma\n",
            "emma → nsubj → woodhouse\n",
            "woodhouse → ccomp → austen\n",
            "handsome → amod → disposition\n",
            "clever → amod → disposition\n",
            "rich → amod → disposition\n",
            "comfortable → amod → disposition\n",
            "home → npadvmod → happy\n",
            "happy → amod → disposition\n",
            "disposition → dobj → woodhouse\n",
            "seemed → ROOT → seemed\n",
            "unite → xcomp → seemed\n",
            "best → amod → blessing\n",
            "blessing → compound → existence\n",
            "existence → nsubj → lived\n",
            "lived → conj → seemed\n",
            "nearly → advmod → year\n",
            "year → npadvmod → lived\n",
            "world → nmod → distress\n",
            "little → amod → distress\n",
            "distress → npadvmod → lived\n",
            "vex → npadvmod → lived\n",
            "wa → advmod → youngest\n",
            "youngest → amod → daughter\n",
            "two → nummod → daughter\n",
            "daughter → ROOT → daughter\n",
            "\n",
            "Named Entities:\n",
            "chapter emma (LAW)\n",
            "nearly year (DATE)\n",
            "two (CARDINAL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "2V-dJtXikcev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}